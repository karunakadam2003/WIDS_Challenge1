{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5t-tqiSvG56",
        "outputId": "138a660e-d8dc-4786-c308-4b3daad13d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.3-cp310-cp310-manylinux2014_x86_64.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install shap\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and the target variable is 'DiagPeriodL90D'\n",
        "df = pd.read_csv('train_final3.csv')\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop(['DiagPeriodL90D'], axis=1)\n",
        "y = df['DiagPeriodL90D']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Impute missing values with the mean (you can choose a different strategy)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# Initialize Gradient Boosting classifier with specified parameters\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    random_state=42,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    min_samples_leaf=1\n",
        ")\n",
        "\n",
        "# Fit the model to the training data\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions on the test set\n",
        "gb_predictions = gb_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_gb = accuracy_score(y_test, gb_predictions)\n",
        "print(f'Gradient Boosting Accuracy: {accuracy_gb}')\n",
        "\n",
        "explainer = shap.Explainer(gb_model, X)\n",
        "shap_values = explainer(X)"
      ],
      "metadata": {
        "id": "MKZNR0-cvZ-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values)"
      ],
      "metadata": {
        "id": "8QgYoOWevc4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values, max_display=83)"
      ],
      "metadata": {
        "id": "Er_AwDu0vfmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values[2], max_display=83)"
      ],
      "metadata": {
        "id": "umwHQNkHvlBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meta learner = xgboost (all features trained)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and the target variable is 'DiagPeriodL90D'\n",
        "df = pd.read_csv('train_final3.csv')\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop(['DiagPeriodL90D'], axis=1)\n",
        "y = df['DiagPeriodL90D']\n",
        "# Split the original training data into training and holdout sets\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Create individual models\n",
        "lgbm_params = {'n_estimators': [9, 10, 11], 'max_leaves': [31, 32, 33], 'learning_rate': [0.2, 0.3]}\n",
        "catboost_params = {'iterations': [50, 100, 150], 'depth': [6, 7, 8], 'learning_rate': [0.1, 0.2, 0.3]}\n",
        "gb_params = {'n_estimators': [100, 150, 200], 'learning_rate': [0.1, 0.2, 0.3]}\n",
        "ada_params = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.2]}\n",
        "\n",
        "lgbm_grid = GridSearchCV(LGBMClassifier(random_state=42), lgbm_params, cv=3)\n",
        "catboost_grid = GridSearchCV(CatBoostClassifier(random_state=42), catboost_params, cv=3)\n",
        "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_params, cv=3)\n",
        "ada_grid = GridSearchCV(AdaBoostClassifier(random_state=42), ada_params, cv=3)\n",
        "\n",
        "lgbm_grid.fit(X_train, y_train)\n",
        "catboost_grid.fit(X_train, y_train)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "ada_grid.fit(X_train, y_train)\n",
        "\n",
        "lgbm_model = lgbm_grid.best_estimator_\n",
        "catboost_model = catboost_grid.best_estimator_\n",
        "gb_model = gb_grid.best_estimator_\n",
        "ada_model = ada_grid.best_estimator_\n",
        "\n",
        "# Train individual models on the training set\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "catboost_model.fit(X_train, y_train)\n",
        "gb_model.fit(X_train, y_train)\n",
        "ada_model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions on the holdout set\n",
        "proba_lgbm_holdout = lgbm_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_catboost_holdout = catboost_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_gb_holdout = gb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_ada_holdout = ada_model.predict_proba(X_holdout)[:, 1]\n",
        "\n",
        "# Combine predictions into a DataFrame\n",
        "ensemble_predictions_holdout = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_holdout,\n",
        "    'CatBoost': proba_catboost_holdout,\n",
        "    'GB': proba_gb_holdout,\n",
        "    'ADA': proba_ada_holdout\n",
        "})\n",
        "\n",
        "# Train XGBClassifier as a meta-learner\n",
        "meta_learner = XGBClassifier(iterations=100, depth=6, learning_rate=0.1, random_seed=42)\n",
        "\n",
        "# Train CatBoostClassifier as a meta-learner on the holdout set\n",
        "meta_learner.fit(ensemble_predictions_holdout, y_holdout)\n",
        "holdout_pred = meta_learner.predict(ensemble_predictions_holdout)\n",
        "# Evaluate accuracy of the meta-learner\n",
        "accuracy_meta = accuracy_score(y_holdout, holdout_pred)\n",
        "print(f'Meta-Learner (XGBoost) Accuracy: {accuracy_meta}')\n",
        "\n",
        "test_data = pd.read_csv('test_final3.csv')\n",
        "\n",
        "# Clean feature names in test data (if needed)\n",
        "test_data.columns = [re.sub(r'[^\\w\\s]', '', col) for col in test_data.columns]\n",
        "\n",
        "# Now, generate predictions on your test set using the trained models\n",
        "proba_lgbm_test = lgbm_model.predict_proba(test_data)[:, 1]\n",
        "proba_catboost_test = catboost_model.predict_proba(test_data)[:, 1]  # Change: Use CatBoost model\n",
        "proba_gb_test = gb_model.predict_proba(test_data)[:, 1]\n",
        "proba_ada_test = ada_model.predict_proba(test_data)[:, 1]\n",
        "\n",
        "# Combine test set predictions into a DataFrame\n",
        "ensemble_predictions_test = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_test,\n",
        "    'CatBoost': proba_catboost_test,  # Change: Use CatBoost model\n",
        "    'GB': proba_gb_test,\n",
        "    'ADA': proba_ada_test\n",
        "})\n",
        "\n",
        "# Generate predictions from the meta-learner on the test set\n",
        "test_prob_predictions = meta_learner.predict_proba(ensemble_predictions_test)[:, 1]\n",
        "\n",
        "# # Round off the probabilities to 1 decimal place\n",
        "# rounded_prob_predictions = [round(prob, 1) for prob in test_prob_predictions]\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'patient_id': test_data['patient_id'],\n",
        "    'DiagPeriodL90D': test_prob_predictions\n",
        "})\n",
        "\n",
        "# Write the submission DataFrame to a CSV file\n",
        "submission_df.to_csv('submission_pnrmx3.csv', index=False)\n",
        "\n",
        "# Download the submission file\n",
        "from google.colab import files\n",
        "files.download('submission_pnrmx3.csv')\n"
      ],
      "metadata": {
        "id": "vgCxOonzvlED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meta learner = lgbm (all features trained)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import re\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and the target variable is 'DiagPeriodL90D'\n",
        "df = pd.read_csv('train_final3.csv')\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop(['DiagPeriodL90D'], axis=1)\n",
        "y = df['DiagPeriodL90D']\n",
        "# Split the original training data into training and holdout sets\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Create individual models\n",
        "cat_params = {'iterations': [100, 150, 200], 'depth': [6, 7, 8], 'learning_rate': [0.1, 0.2, 0.3]}\n",
        "xgb_params = {'n_estimators': [18, 19, 20], 'max_depth': [3, 4, 5], 'learning_rate': [0.2, 0.3]}\n",
        "gb_params = {'n_estimators': [100, 150, 200], 'learning_rate': [0.1, 0.2, 0.3]}\n",
        "ada_params = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.2]}\n",
        "\n",
        "cat_grid = GridSearchCV(CatBoostClassifier(random_state=42), cat_params, cv=3)\n",
        "xgb_grid = GridSearchCV(XGBClassifier(random_state=42, booster='gbtree'), xgb_params, cv=3)\n",
        "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_params, cv=3)\n",
        "ada_grid = GridSearchCV(AdaBoostClassifier(random_state=42), ada_params, cv=3)\n",
        "\n",
        "cat_grid.fit(X_train, y_train)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "ada_grid.fit(X_train, y_train)\n",
        "\n",
        "cat_model = cat_grid.best_estimator_\n",
        "xgb_model = xgb_grid.best_estimator_\n",
        "gb_model = gb_grid.best_estimator_\n",
        "ada_model = ada_grid.best_estimator_\n",
        "\n",
        "# Train individual models on the training set\n",
        "cat_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "gb_model.fit(X_train, y_train)\n",
        "ada_model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions on the holdout set\n",
        "proba_cat_holdout = cat_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_xgb_holdout = xgb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_gb_holdout = gb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_ada_holdout = ada_model.predict_proba(X_holdout)[:, 1]\n",
        "\n",
        "# Combine predictions into a DataFrame\n",
        "ensemble_predictions_holdout = pd.DataFrame({\n",
        "    'CatBoost': proba_cat_holdout,\n",
        "    'XGB': proba_xgb_holdout,\n",
        "    'GB': proba_gb_holdout,\n",
        "    'ADA': proba_ada_holdout\n",
        "})\n",
        "# Train LightGBM as a meta-learner\n",
        "meta_learner = LGBMClassifier(n_estimators=100, max_leaves=31, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train LightGBM as a meta-learner on the holdout set\n",
        "meta_learner.fit(ensemble_predictions_holdout, y_holdout)\n",
        "holdout_pred = meta_learner.predict(ensemble_predictions_holdout)\n",
        "# Evaluate accuracy of the meta-learner\n",
        "accuracy_meta = accuracy_score(y_holdout, holdout_pred)\n",
        "print(f'Meta-Learner (LightGBM) Accuracy: {accuracy_meta}')\n",
        "\n",
        "test_data = pd.read_csv('test_final3.csv')\n",
        "\n",
        "# Clean feature names in test data (if needed)\n",
        "test_data.columns = [re.sub(r'[^\\w\\s]', '', col) for col in test_data.columns]\n",
        "\n",
        "# Now, generate predictions on your test set using the trained models\n",
        "proba_cat_test = cat_model.predict_proba(test_data)[:, 1]\n",
        "proba_xgb_test = xgb_model.predict_proba(test_data)[:, 1]\n",
        "proba_gb_test = gb_model.predict_proba(test_data)[:, 1]\n",
        "proba_ada_test = ada_model.predict_proba(test_data)[:, 1]\n",
        "\n",
        "# Combine test set predictions into a DataFrame\n",
        "ensemble_predictions_test = pd.DataFrame({\n",
        "    'CatBoost': proba_cat_test,\n",
        "    'XGB': proba_xgb_test,\n",
        "    'GB': proba_gb_test,\n",
        "    'ADA': proba_ada_test\n",
        "})\n",
        "\n",
        "# Generate predictions from the meta-learner on the test set\n",
        "test_prob_predictions = meta_learner.predict_proba(ensemble_predictions_test)[:, 1]\n",
        "\n",
        "# # Round off the probabilities to 1 decimal place\n",
        "# rounded_prob_predictions = [round(prob, 1) for prob in test_prob_predictions]\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'patient_id': test_data['patient_id'],\n",
        "    'DiagPeriodL90D': test_prob_predictions\n",
        "})\n",
        "\n",
        "# Write the submission DataFrame to a CSV file\n",
        "submission_df.to_csv('submission_pnrml3.csv', index=False)\n",
        "\n",
        "# Download the submission file\n",
        "from google.colab import files\n",
        "files.download('submission_pnrml3.csv')\n"
      ],
      "metadata": {
        "id": "oyRqiBpOvlGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meta learner = catboost (all features trained)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import re\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and the target variable is 'DiagPeriodL90D'\n",
        "df = pd.read_csv('train_final3.csv')\n",
        "\n",
        "# Extract the feature 'breast_cancer_code_encoded' and the target variable\n",
        "X = df.drop(['DiagPeriodL90D'], axis=1)\n",
        "y = df['DiagPeriodL90D']\n",
        "\n",
        "# Split the data into training and holdout sets\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual models\n",
        "lgbm_params = {'n_estimators': [9, 10, 11], 'max_leaves': [31, 32, 33], 'learning_rate': [0.2, 0.3]}\n",
        "xgb_params = {'n_estimators': [18, 19, 20], 'max_depth': [3, 4, 5], 'learning_rate': [0.2, 0.3]}\n",
        "gb_params = {'n_estimators': [100, 150, 200], 'learning_rate': [0.1, 0.2, 0.3]}\n",
        "ada_params = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.2]}\n",
        "\n",
        "lgbm_grid = GridSearchCV(LGBMClassifier(random_state=42), lgbm_params, cv=3)\n",
        "xgb_grid = GridSearchCV(XGBClassifier(random_state=42, booster='gbtree'), xgb_params, cv=3)\n",
        "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_params, cv=3)\n",
        "ada_grid = GridSearchCV(AdaBoostClassifier(random_state=42), ada_params, cv=3)\n",
        "\n",
        "lgbm_grid.fit(X_train, y_train)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "ada_grid.fit(X_train, y_train)\n",
        "\n",
        "lgbm_model = lgbm_grid.best_estimator_\n",
        "xgb_model = xgb_grid.best_estimator_\n",
        "gb_model = gb_grid.best_estimator_\n",
        "ada_model = ada_grid.best_estimator_\n",
        "\n",
        "# Train individual models on the training set\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "gb_model.fit(X_train, y_train)\n",
        "ada_model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions on the holdout set\n",
        "proba_lgbm_holdout = lgbm_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_xgb_holdout = xgb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_gb_holdout = gb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_ada_holdout = ada_model.predict_proba(X_holdout)[:, 1]\n",
        "\n",
        "# Combine predictions into a DataFrame\n",
        "ensemble_predictions_holdout = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_holdout,\n",
        "    'XGB': proba_xgb_holdout,\n",
        "    'GB': proba_gb_holdout,\n",
        "    'ADA': proba_ada_holdout\n",
        "})\n",
        "\n",
        "# Train CatBoostClassifier as a meta-learner on the holdout set\n",
        "meta_learner = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, random_seed=42)\n",
        "meta_learner.fit(ensemble_predictions_holdout, y_holdout)\n",
        "holdout_pred = meta_learner.predict(ensemble_predictions_holdout)\n",
        "# Evaluate accuracy of the meta-learner\n",
        "accuracy_meta = accuracy_score(y_holdout, holdout_pred)\n",
        "print(f'Meta-Learner (CatBoost) Accuracy: {accuracy_meta}')\n",
        "\n",
        "# Generate predictions on the test set using the trained models\n",
        "test_data = pd.read_csv('test_final3.csv')\n",
        "\n",
        "proba_lgbm_test = lgbm_model.predict_proba(test_data)[:, 1]\n",
        "proba_xgb_test = xgb_model.predict_proba(test_data)[:, 1]\n",
        "proba_gb_test = gb_model.predict_proba(test_data)[:, 1]\n",
        "proba_ada_test = ada_model.predict_proba(test_data)[:, 1]\n",
        "\n",
        "# Combine test set predictions into a DataFrame\n",
        "ensemble_predictions_test = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_test,\n",
        "    'XGB': proba_xgb_test,\n",
        "    'GB': proba_gb_test,\n",
        "    'ADA': proba_ada_test\n",
        "})\n",
        "\n",
        "# Generate predictions from the meta-learner on the test set\n",
        "test_prob_predictions = meta_learner.predict_proba(ensemble_predictions_test)[:, 1]\n",
        "\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'patient_id': test_data['patient_id'],\n",
        "    'DiagPeriodL90D': test_prob_predictions\n",
        "})\n",
        "\n",
        "# Write the submission DataFrame to a CSV file\n",
        "submission_df.to_csv('submission_pnrwe.csv', index=False)\n",
        "\n",
        "# Download the submission file\n",
        "from google.colab import files\n",
        "files.download('submission_pnrwe.csv')\n"
      ],
      "metadata": {
        "id": "91sNcvDbvlIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meta learner = gradient boost (all features trained)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import re\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and the target variable is 'DiagPeriodL90D'\n",
        "df = pd.read_csv('train_final3.csv')\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop(['DiagPeriodL90D'], axis=1)\n",
        "y = df['DiagPeriodL90D']\n",
        "# Split the original training data into training and holdout sets\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Create individual models\n",
        "lgbm_params = {'n_estimators': [9, 10, 11], 'max_leaves': [31, 32, 33], 'learning_rate': [0.2, 0.3]}\n",
        "xgb_params = {'n_estimators': [18, 19, 20], 'max_depth': [3, 4, 5], 'learning_rate': [0.2, 0.3]}\n",
        "cat_params = {'iterations': [100, 150, 200], 'depth': [6, 7, 8], 'learning_rate': [0.1, 0.2, 0.3]}\n",
        "ada_params = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.2]}\n",
        "\n",
        "lgbm_grid = GridSearchCV(LGBMClassifier(random_state=42), lgbm_params, cv=3)\n",
        "xgb_grid = GridSearchCV(XGBClassifier(random_state=42, booster='gbtree'), xgb_params, cv=3)\n",
        "cat_grid = GridSearchCV(CatBoostClassifier(random_state=42), cat_params, cv=3)\n",
        "ada_grid = GridSearchCV(AdaBoostClassifier(random_state=42), ada_params, cv=3)\n",
        "\n",
        "lgbm_grid.fit(X_train, y_train)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "cat_grid.fit(X_train, y_train)\n",
        "ada_grid.fit(X_train, y_train)\n",
        "\n",
        "lgbm_model = lgbm_grid.best_estimator_\n",
        "xgb_model = xgb_grid.best_estimator_\n",
        "cat_model = cat_grid.best_estimator_\n",
        "ada_model = ada_grid.best_estimator_\n",
        "\n",
        "# Train individual models on the training set\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "cat_model.fit(X_train, y_train)\n",
        "ada_model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions on the holdout set\n",
        "proba_lgbm_holdout = lgbm_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_xgb_holdout = xgb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_cat_holdout = cat_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_ada_holdout = ada_model.predict_proba(X_holdout)[:, 1]\n",
        "\n",
        "# Combine predictions into a DataFrame\n",
        "ensemble_predictions_holdout = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_holdout,\n",
        "    'XGB': proba_xgb_holdout,\n",
        "    'CAT': proba_cat_holdout,\n",
        "    'ADA': proba_ada_holdout\n",
        "})\n",
        "\n",
        "# Train GradientBoostingClassifier as a meta-learner\n",
        "meta_learner = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train GradientBoostingClassifier as a meta-learner on the holdout set\n",
        "meta_learner.fit(ensemble_predictions_holdout, y_holdout)\n",
        "holdout_pred = meta_learner.predict(ensemble_predictions_holdout)\n",
        "# Evaluate accuracy of the meta-learner\n",
        "accuracy_meta = accuracy_score(y_holdout, holdout_pred)\n",
        "print(f'Meta-Learner (GradientBoosting) Accuracy: {accuracy_meta}')\n",
        "\n",
        "test_data = pd.read_csv('test_final3.csv')\n",
        "\n",
        "# Clean feature names in test data (if needed)\n",
        "test_data.columns = [re.sub(r'[^\\w\\s]', '', col) for col in test_data.columns]\n",
        "\n",
        "# Now, generate predictions on your test set using the trained models\n",
        "proba_lgbm_test = lgbm_model.predict_proba(test_data)[:, 1]\n",
        "proba_xgb_test = xgb_model.predict_proba(test_data)[:, 1]\n",
        "proba_cat_test = cat_model.predict_proba(test_data)[:, 1]\n",
        "proba_ada_test = ada_model.predict_proba(test_data)[:, 1]\n",
        "\n",
        "# Combine test set predictions into a DataFrame\n",
        "ensemble_predictions_test = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_test,\n",
        "    'XGB': proba_xgb_test,\n",
        "    'CAT': proba_cat_test,\n",
        "    'ADA': proba_ada_test\n",
        "})\n",
        "\n",
        "# Generate predictions from the meta-learner on the test set\n",
        "test_prob_predictions = meta_learner.predict_proba(ensemble_predictions_test)[:, 1]\n",
        "\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'patient_id': test_data['patient_id'],\n",
        "    'DiagPeriodL90D': test_prob_predictions\n",
        "})\n",
        "\n",
        "# Write the submission DataFrame to a CSV file\n",
        "submission_df.to_csv('submission_pnrmg3.csv', index=False)\n",
        "\n",
        "# Download the submission file\n",
        "from google.colab import files\n",
        "files.download('submission_pnrmg3.csv')\n"
      ],
      "metadata": {
        "id": "uVKlG1X9vlLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meta learner = ada boost  (all features trained)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import re\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and the target variable is 'DiagPeriodL90D'\n",
        "df = pd.read_csv('train_final3.csv')\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop(['DiagPeriodL90D'], axis=1)\n",
        "y = df['DiagPeriodL90D']\n",
        "# Split the original training data into training and holdout sets\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual models\n",
        "lgbm_params = {'n_estimators': [9, 10, 11], 'max_leaves': [31, 32, 33], 'learning_rate': [0.2, 0.3]}\n",
        "xgb_params = {'n_estimators': [18, 19, 20], 'max_depth': [3, 4, 5], 'learning_rate': [0.2, 0.3]}\n",
        "gb_params = {'n_estimators': [100, 150, 200], 'learning_rate': [0.1, 0.2, 0.3]}\n",
        "cat_params = {'iterations': [50, 100, 150], 'depth': [6, 7, 8], 'learning_rate': [0.01, 0.1, 0.2]}\n",
        "\n",
        "lgbm_grid = GridSearchCV(LGBMClassifier(random_state=42), lgbm_params, cv=3)\n",
        "xgb_grid = GridSearchCV(XGBClassifier(random_state=42, booster='gbtree'), xgb_params, cv=3)\n",
        "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_params, cv=3)\n",
        "cat_grid = GridSearchCV(CatBoostClassifier(random_state=42), cat_params, cv=3)\n",
        "\n",
        "lgbm_grid.fit(X_train, y_train)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "cat_grid.fit(X_train, y_train)\n",
        "\n",
        "lgbm_model = lgbm_grid.best_estimator_\n",
        "xgb_model = xgb_grid.best_estimator_\n",
        "gb_model = gb_grid.best_estimator_\n",
        "cat_model = cat_grid.best_estimator_\n",
        "\n",
        "# Train individual models on the training set\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "gb_model.fit(X_train, y_train)\n",
        "cat_model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions on the holdout set\n",
        "proba_lgbm_holdout = lgbm_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_xgb_holdout = xgb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_gb_holdout = gb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_cat_holdout = cat_model.predict_proba(X_holdout)[:, 1]\n",
        "\n",
        "# Combine predictions into a DataFrame\n",
        "ensemble_predictions_holdout = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_holdout,\n",
        "    'XGB': proba_xgb_holdout,\n",
        "    'GB': proba_gb_holdout,\n",
        "    'CAT': proba_cat_holdout\n",
        "})\n",
        "# Train AdaBoostClassifier as a meta-learner\n",
        "meta_learner = AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train AdaBoostClassifier as a meta-learner on the holdout set\n",
        "meta_learner.fit(ensemble_predictions_holdout, y_holdout)\n",
        "holdout_pred = meta_learner.predict(ensemble_predictions_holdout)\n",
        "# Evaluate accuracy of the meta-learner\n",
        "accuracy_meta = accuracy_score(y_holdout, holdout_pred)\n",
        "print(f'Meta-Learner (AdaBoost) Accuracy: {accuracy_meta}')\n",
        "\n",
        "test_data = pd.read_csv('test_final3.csv')\n",
        "\n",
        "# Clean feature names in test data (if needed)\n",
        "test_data.columns = [re.sub(r'[^\\w\\s]', '', col) for col in test_data.columns]\n",
        "\n",
        "# Now, generate predictions on your test set using the trained models\n",
        "proba_lgbm_test = lgbm_model.predict_proba(test_data)[:, 1]\n",
        "proba_xgb_test = xgb_model.predict_proba(test_data)[:, 1]\n",
        "proba_gb_test = gb_model.predict_proba(test_data)[:, 1]\n",
        "proba_cat_test = cat_model.predict_proba(test_data)[:, 1]\n",
        "\n",
        "# Combine test set predictions into a DataFrame\n",
        "ensemble_predictions_test = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_test,\n",
        "    'XGB': proba_xgb_test,\n",
        "    'GB': proba_gb_test,\n",
        "    'CAT': proba_cat_test\n",
        "})\n",
        "\n",
        "# Generate predictions from the meta-learner on the test set\n",
        "test_prob_predictions = meta_learner.predict_proba(ensemble_predictions_test)[:, 1]\n",
        "\n",
        "# # Round off the probabilities to 1 decimal place\n",
        "# rounded_prob_predictions = [round(prob, 1) for prob in test_prob_predictions]\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'patient_id': test_data['patient_id'],\n",
        "    'DiagPeriodL90D': test_prob_predictions\n",
        "})\n",
        "\n",
        "# Write the submission DataFrame to a CSV file\n",
        "submission_df.to_csv('submission_pnrma3.csv', index=False)\n",
        "\n",
        "# Download the submission file\n",
        "from google.colab import files\n",
        "files.download('submission_pnrma3.csv')\n"
      ],
      "metadata": {
        "id": "nNJMBf-ewSmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top shap values trained on meta learner adaboost(score: top3=0.808, top4=0.807, top9=0.801, top2=0.778)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import re\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and the target variable is 'DiagPeriodL90D'\n",
        "df = pd.read_csv('train_final3.csv')\n",
        "# List of selected columns\n",
        "selected_columns = ['patient_age', 'breast_cancer_diagnosis_code_encoded', 'metastatic_cancer_diagnosis_code_encoded']\n",
        "\n",
        "# Split the data into features (X) and target variable (y) using only selected columns\n",
        "X = df[selected_columns]\n",
        "y = df['DiagPeriodL90D']\n",
        "# Split the original training data into training and holdout sets\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create individual models\n",
        "lgbm_params = {'n_estimators': [9, 10, 11], 'max_leaves': [31, 32, 33], 'learning_rate': [0.2, 0.3]}\n",
        "xgb_params = {'n_estimators': [18, 19, 20], 'max_depth': [3, 4, 5], 'learning_rate': [0.2, 0.3]}\n",
        "gb_params = {'n_estimators': [100, 150, 200], 'learning_rate': [0.1, 0.2, 0.3]}\n",
        "cat_params = {'iterations': [50, 100, 150], 'depth': [6, 7, 8], 'learning_rate': [0.01, 0.1, 0.2]}\n",
        "\n",
        "lgbm_grid = GridSearchCV(LGBMClassifier(random_state=42), lgbm_params, cv=3)\n",
        "xgb_grid = GridSearchCV(XGBClassifier(random_state=42, booster='gbtree'), xgb_params, cv=3)\n",
        "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_params, cv=3)\n",
        "cat_grid = GridSearchCV(CatBoostClassifier(random_state=42), cat_params, cv=3)\n",
        "\n",
        "lgbm_grid.fit(X_train, y_train)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "cat_grid.fit(X_train, y_train)\n",
        "\n",
        "lgbm_model = lgbm_grid.best_estimator_\n",
        "xgb_model = xgb_grid.best_estimator_\n",
        "gb_model = gb_grid.best_estimator_\n",
        "cat_model = cat_grid.best_estimator_\n",
        "\n",
        "# Train individual models on the training set\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "gb_model.fit(X_train, y_train)\n",
        "cat_model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions on the holdout set\n",
        "proba_lgbm_holdout = lgbm_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_xgb_holdout = xgb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_gb_holdout = gb_model.predict_proba(X_holdout)[:, 1]\n",
        "proba_cat_holdout = cat_model.predict_proba(X_holdout)[:, 1]\n",
        "\n",
        "# Combine predictions into a DataFrame\n",
        "ensemble_predictions_holdout = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_holdout,\n",
        "    'XGB': proba_xgb_holdout,\n",
        "    'GB': proba_gb_holdout,\n",
        "    'CAT': proba_cat_holdout\n",
        "})\n",
        "# Train AdaBoostClassifier as a meta-learner\n",
        "meta_learner = AdaBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train AdaBoostClassifier as a meta-learner on the holdout set\n",
        "meta_learner.fit(ensemble_predictions_holdout, y_holdout)\n",
        "holdout_pred = meta_learner.predict(ensemble_predictions_holdout)\n",
        "# Evaluate accuracy of the meta-learner\n",
        "accuracy_meta = accuracy_score(y_holdout, holdout_pred)\n",
        "print(f'Meta-Learner (AdaBoost) Accuracy: {accuracy_meta}')\n",
        "\n",
        "test_data = pd.read_csv('test_final3.csv')\n",
        "\n",
        "# Clean feature names in test data (if needed)\n",
        "test_data.columns = [re.sub(r'[^\\w\\s]', '', col) for col in test_data.columns]\n",
        "\n",
        "# Now, generate predictions on your test set using the trained models\n",
        "proba_lgbm_test = lgbm_model.predict_proba(test_data[selected_columns])[:, 1]\n",
        "proba_xgb_test = xgb_model.predict_proba(test_data[selected_columns])[:, 1]\n",
        "proba_gb_test = gb_model.predict_proba(test_data[selected_columns])[:, 1]\n",
        "proba_cat_test = cat_model.predict_proba(test_data[selected_columns])[:, 1]\n",
        "\n",
        "# Combine test set predictions into a DataFrame\n",
        "ensemble_predictions_test = pd.DataFrame({\n",
        "    'LGBM': proba_lgbm_test,\n",
        "    'XGB': proba_xgb_test,\n",
        "    'GB': proba_gb_test,\n",
        "    'CAT': proba_cat_test\n",
        "})\n",
        "\n",
        "# Generate predictions from the meta-learner on the test set\n",
        "test_prob_predictions = meta_learner.predict_proba(ensemble_predictions_test)[:, 1]\n",
        "\n",
        "# # Round off the probabilities to 1 decimal place\n",
        "# rounded_prob_predictions = [round(prob, 1) for prob in test_prob_predictions]\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'patient_id': test_data['patient_id'],\n",
        "    'DiagPeriodL90D': test_prob_predictions\n",
        "})\n",
        "\n",
        "# Write the submission DataFrame to a CSV file\n",
        "submission_df.to_csv('submission_pnraot3.csv', index=False)\n",
        "\n",
        "# Download the submission file\n",
        "from google.colab import files\n",
        "files.download('submission_pnraot3.csv')"
      ],
      "metadata": {
        "id": "kbiLNgMUwSol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top shap values trained on meta learner xgboost"
      ],
      "metadata": {
        "id": "MNxP1OohwSqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top shap values trained on meta learner catboost"
      ],
      "metadata": {
        "id": "isKg6dXKwSt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top shap values trained on meta learner lgbm"
      ],
      "metadata": {
        "id": "6FdWUypdwzS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top shap values trained on meta learner gradient boost"
      ],
      "metadata": {
        "id": "DVak-RDPwzeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}