{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer Wisconsin dataset\n",
        "data = pd.read_csv('train_final2.csv')\n",
        "X = data.drop(['DiagPeriodL90D'], axis=1)\n",
        "y = data['DiagPeriodL90D']\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Principal Component Analysis (PCA)\n",
        "pca = PCA(n_components=25)  # Choose the number of components\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Singular Value Decomposition (SVD)\n",
        "svd = TruncatedSVD(n_components=25)  # Choose the number of components\n",
        "X_train_svd = svd.fit_transform(X_train_scaled)\n",
        "X_test_svd = svd.transform(X_test_scaled)\n",
        "\n",
        "# Chi-Square (Chi2)\n",
        "chi2_selector = SelectKBest(chi2, k=25)  # Choose the number of top features (k)\n",
        "X_train_chi2 = chi2_selector.fit_transform(X_train, y_train)\n",
        "X_test_chi2 = chi2_selector.transform(X_test)\n",
        "\n",
        "# Train a classifier on each feature selection method\n",
        "clf_pca = RandomForestClassifier(random_state=42)\n",
        "clf_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = clf_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(f'Accuracy with PCA: {accuracy_pca}')\n",
        "\n",
        "clf_svd = RandomForestClassifier(random_state=42)\n",
        "clf_svd.fit(X_train_svd, y_train)\n",
        "y_pred_svd = clf_svd.predict(X_test_svd)\n",
        "accuracy_svd = accuracy_score(y_test, y_pred_svd)\n",
        "print(f'Accuracy with SVD: {accuracy_svd}')\n",
        "\n",
        "clf_chi2 = RandomForestClassifier(random_state=42)\n",
        "clf_chi2.fit(X_train_chi2, y_train)\n",
        "y_pred_chi2 = clf_chi2.predict(X_test_chi2)\n",
        "accuracy_chi2 = accuracy_score(y_test, y_pred_chi2)\n",
        "print(f'Accuracy with Chi2: {accuracy_chi2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Voo-Ll7dA3r4",
        "outputId": "809d8e79-2fd9-4e77-f302-5867ff725496"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA: 0.6835786212238575\n",
            "Accuracy with SVD: 0.6897753679318358\n",
            "Accuracy with Chi2: 0.6700232378001549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOI0LueSAzzq",
        "outputId": "8d83047b-7404-43f9-d4d8-83893018f7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of RS-SVM model: 0.6885235732009926\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Read the dataset\n",
        "df = pd.read_csv('train_final2.csv')\n",
        "\n",
        "# Step 2: Identify features related to Fine Needle Aspiration (FNA)\n",
        "# Assuming 'FNA_features' is a list of features related to FNA\n",
        "# Step 2: Identify features related to Fine Needle Aspiration (FNA)\n",
        "FNA_features = df.columns[df.columns != 'DiagPeriodL90D'].tolist()\n",
        "  # Define your FNA features\n",
        "\n",
        "# Subset the DataFrame with FNA features\n",
        "df_fna = df[FNA_features + ['DiagPeriodL90D']]  # Include target variable if needed\n",
        "\n",
        "# Step 3: Optionally balance the dataset using SMOTE\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df_fna.drop(['DiagPeriodL90D'], axis=1)\n",
        "y = df_fna['DiagPeriodL90D']\n",
        "\n",
        "# Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Step 4: Apply dimensionality reduction techniques\n",
        "# SVD\n",
        "svd = TruncatedSVD(n_components=20)  # Adjust the number of components as needed\n",
        "X_svd = svd.fit_transform(X_resampled)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=20)  # Adjust the number of components as needed\n",
        "X_pca = pca.fit_transform(X_resampled)\n",
        "\n",
        "# Chi-square\n",
        "selector = SelectKBest(chi2, k=20)  # Adjust k as needed\n",
        "X_chi2 = selector.fit_transform(X_resampled, y_resampled)\n",
        "\n",
        "# Step 5: Apply RS-SVM\n",
        "# Define RS-SVM pipeline\n",
        "rs_svm_pipeline = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
        "\n",
        "# Train RS-SVM on the original dataset\n",
        "rs_svm_pipeline.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Get feature importances from RS-SVM\n",
        "rs_svm_importances = rs_svm_pipeline.named_steps['svc'].coef_\n",
        "\n",
        "# Optionally, you can use feature importances to select top features.\n",
        "\n",
        "# Evaluate the accuracy of the RS-SVM model\n",
        "accuracy_rs_svm = accuracy_score(y_resampled, rs_svm_pipeline.predict(X_resampled))\n",
        "print(\"Accuracy of RS-SVM model:\", accuracy_rs_svm)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('train_final2.csv')\n",
        "X = data.drop(['DiagPeriodL90D'], axis=1)\n",
        "y = data['DiagPeriodL90D']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Principal Component Analysis (PCA)\n",
        "pca = PCA(n_components= 50)  # Choose the number of components\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Singular Value Decomposition (SVD)\n",
        "svd = TruncatedSVD(n_components=48)  # Choose the number of components\n",
        "X_train_svd = svd.fit_transform(X_train_scaled)\n",
        "X_test_svd = svd.transform(X_test_scaled)\n",
        "\n",
        "# Chi-Square (Chi2)\n",
        "chi2_selector = SelectKBest(chi2, k=49)  # Choose the number of top features (k)\n",
        "X_train_chi2 = chi2_selector.fit_transform(X_train, y_train)\n",
        "X_test_chi2 = chi2_selector.transform(X_test)\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "    'LGBM': LGBMClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize variables to keep track of the best classifier and its accuracy\n",
        "best_clf_name = None\n",
        "best_accuracy = 0\n",
        "\n",
        "# Train models and keep track of the best one\n",
        "for method, X_train_method, X_test_method in zip(['PCA', 'SVD', 'Chi2'],\n",
        "                                                 [X_train_pca, X_train_svd, X_train_chi2],\n",
        "                                                 [X_test_pca, X_test_svd, X_test_chi2]):\n",
        "    print(f\"Results with {method}:\")\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        clf.fit(X_train_method, y_train)\n",
        "        y_pred = clf.predict(X_test_method)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f'{clf_name} - Accuracy: {accuracy}')\n",
        "\n",
        "        # Update the best classifier if the current one has higher accuracy\n",
        "        if accuracy > best_accuracy:\n",
        "            best_clf_name = clf_name\n",
        "            best_accuracy = accuracy\n",
        "    print()\n",
        "\n",
        "# Load test dataset\n",
        "test_data = pd.read_csv('test_final2.csv')\n",
        "# Use the best classifier to make predictions on the test data\n",
        "best_clf = classifiers[best_clf_name]\n",
        "best_clf.fit(X_train, y_train)  # Train on the entire training data\n",
        "\n",
        "# Use the best classifier to predict probabilities on the test data\n",
        "test_prob_predictions = best_clf.predict_proba(test_data)[:, 1]\n",
        "\n",
        "# Round off the probabilities to 1 decimal place\n",
        "rounded_prob_predictions = [round(prob, 1) for prob in test_prob_predictions]\n",
        "\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'patient_id': test_data['patient_id'],\n",
        "    'DiagPeriodL90D': rounded_prob_predictions\n",
        "})\n",
        "\n",
        "# Write the submission DataFrame to a CSV file\n",
        "# submission_df.to_csv('submission_FS3.csv', index=False)\n",
        "submission_df.to_csv('submission_FS3_2.csv', index=False)\n",
        "\n",
        "\n",
        "# Download the submission file\n",
        "from google.colab import files\n",
        "# files.download('submission_FS3.csv')\n",
        "files.download('submission_FS3_2.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F0LMgM_yjtPY",
        "outputId": "63cef032-6363-4420-fd1e-0aa85086b3c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results with PCA:\n",
            "RandomForest - Accuracy: 0.7099147947327653\n",
            "GradientBoosting - Accuracy: 0.7327652982184353\n",
            "[LightGBM] [Info] Number of positive: 6443, number of negative: 3881\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004114 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 12750\n",
            "[LightGBM] [Info] Number of data points in the train set: 10324, number of used features: 50\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624080 -> initscore=0.506901\n",
            "[LightGBM] [Info] Start training from score 0.506901\n",
            "LGBM - Accuracy: 0.7331525948876839\n",
            "XGBoost - Accuracy: 0.710302091402014\n",
            "\n",
            "Results with SVD:\n",
            "RandomForest - Accuracy: 0.7013942680092952\n",
            "GradientBoosting - Accuracy: 0.7300542215336948\n",
            "[LightGBM] [Info] Number of positive: 6443, number of negative: 3881\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004230 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 12240\n",
            "[LightGBM] [Info] Number of data points in the train set: 10324, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624080 -> initscore=0.506901\n",
            "[LightGBM] [Info] Start training from score 0.506901\n",
            "LGBM - Accuracy: 0.7234701781564679\n",
            "XGBoost - Accuracy: 0.7044926413632843\n",
            "\n",
            "Results with Chi2:\n",
            "RandomForest - Accuracy: 0.7316034082106894\n",
            "GradientBoosting - Accuracy: 0.7591014717273431\n",
            "[LightGBM] [Info] Number of positive: 6443, number of negative: 3881\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004179 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 10312\n",
            "[LightGBM] [Info] Number of data points in the train set: 10324, number of used features: 49\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624080 -> initscore=0.506901\n",
            "[LightGBM] [Info] Start training from score 0.506901\n",
            "LGBM - Accuracy: 0.7525174283501161\n",
            "XGBoost - Accuracy: 0.7323780015491866\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- DiagPeriodL90D\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2b528bef33c0>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Use the best classifier to predict probabilities on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mtest_prob_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Round off the probabilities to 1 decimal place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupport\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \"\"\"\n\u001b[0;32m-> 1355\u001b[0;31m         \u001b[0mraw_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_prediction_to_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \"\"\"\n\u001b[0;32m-> 1261\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m   1262\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     def _validate_data(\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- DiagPeriodL90D\n"
          ]
        }
      ]
    }
  ]
}