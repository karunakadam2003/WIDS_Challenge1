{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dsTdjq6XIe9q",
        "outputId": "9b2c3bde-615c-474d-b0ea-51084e38e8d4"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Data\n",
        "train_data = pd.read_csv('training.csv')\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(train_data.info())\n",
        "\n",
        "# Summary Statistics\n",
        "print(train_data.describe())\n",
        "\n",
        "# Check Missing Values\n",
        "print(train_data.isnull().sum())\n",
        "\n",
        "\n",
        "corr_df = train_data.drop(['patient_id'],axis=1).select_dtypes(exclude='object').corr()\n",
        "\n",
        "# only looking at correlations showing a coefficient higher than 0.01\n",
        "corr_df = corr_df[abs(corr_df)>0.005].dropna(how='any',axis=0)\n",
        "corr_df = corr_df[abs(corr_df)>0.005].dropna(how='any',axis=1)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
        "sns.heatmap(corr_df)\n",
        "\n",
        "# Distribution of Target Variable\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='DiagPeriodL90D', data=train_data)\n",
        "plt.title('Distribution of Diagnosis Period Less Than 90 Days')\n",
        "plt.xlabel('Diagnosis Period Less Than 90 Days')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Explore Categorical Variables\n",
        "categorical_variables = ['patient_race', 'payer_type', 'patient_state', 'patient_gender']\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, variable in enumerate(categorical_variables, 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    sns.countplot(x=variable, data=train_data)\n",
        "    plt.title(f'Distribution of {variable}')\n",
        "    plt.xlabel(variable)\n",
        "    plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Correlation Analysis\n",
        "correlation_matrix = train_data.corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Explore Time-Related Variables\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x='metastatic_first_novel_treatment_type', data=train_data)\n",
        "plt.title('Distribution of First Novel Treatment Types')\n",
        "plt.xlabel('First Novel Treatment Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Geographic Analysis\n",
        "plt.figure(figsize=(18, 6))\n",
        "sns.countplot(x='patient_state', data=train_data)\n",
        "plt.title('Distribution of Patients Across States')\n",
        "plt.xlabel('State')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Feature Engineering (if needed)\n",
        "# Outlier Detection and Handling\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='DiagPeriodL90D', y='patient_age', data=train_data)\n",
        "plt.title('Outliers in Patient Age by Diagnosis Period Less Than 90 Days')\n",
        "plt.xlabel('Diagnosis Period Less Than 90 Days')\n",
        "plt.ylabel('Patient Age')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "fuWm04GmFuGF",
        "outputId": "e6756ee2-b1a6-45e7-cb81-48fa7c44e26d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "train_data = pd.read_csv(\"training.csv\")\n",
        "\n",
        "# Drop unnecessary columns\n",
        "train_data = train_data.drop(['patient_id'], axis=1)\n",
        "\n",
        "# Handle missing values and encode categorical variables using one-hot encoding\n",
        "train_data = pd.get_dummies(train_data, columns=['patient_race', 'payer_type', 'patient_state', 'patient_gender',\n",
        "                                                 'breast_cancer_diagnosis_code', 'breast_cancer_diagnosis_desc',\n",
        "                                                 'metastatic_cancer_diagnosis_code', 'metastatic_first_novel_treatment',\n",
        "                                                 'metastatic_first_novel_treatment_type', 'Region', 'Division'])\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = train_data.drop(columns=[\"DiagPeriodL90D\"])\n",
        "y = train_data[\"DiagPeriodL90D\"]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose a classification model (XGBoost)\n",
        "model = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "# Train the XGBoost model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importance = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame with feature names and their importance scores\n",
        "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the top N important features\n",
        "top_n_features = 10\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(top_n_features))\n",
        "plt.title(f'Top {top_n_features} Important Features')\n",
        "plt.show()\n",
        "\n",
        "# Extract the top N important features\n",
        "top_features = feature_importance_df.head(top_n_features)['Feature'].tolist()\n",
        "\n",
        "# Select only the top features for correlation analysis\n",
        "selected_features = X_train[top_features]\n",
        "\n",
        "# Calculate correlations among the selected features\n",
        "corr_df = selected_features.corr()\n",
        "\n",
        "# Visualize the correlations using a heatmap\n",
        "plt.figure(figsize=(10, 5))\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
        "sns.heatmap(corr_df, annot=True, fmt=\".2f\", cmap=\"coolwarm\", mask=mask)\n",
        "plt.title('Correlation Heatmap of Top Important Features')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCA2pSW8Hbfr"
      },
      "source": [
        "# **Univariate Feature Selection**:\n",
        "\n",
        "SelectKBest: Select the top k features based on univariate statistical tests.\n",
        "SelectPercentile: Select the top features based on a percentage of the highest scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQeb9dgKHhmQ",
        "outputId": "16ceb454-6f6c-451f-ef3b-b9d2e877800a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data (replace 'your_dataset.csv' with your actual dataset)\n",
        "train_data = pd.read_csv(\"training.csv\")\n",
        "\n",
        "# Drop the patient_id column\n",
        "train_data = train_data.drop(['patient_id'], axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "# Separate numerical and categorical features\n",
        "numerical_features = train_data.select_dtypes(include=['float64']).columns\n",
        "categorical_features = train_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "train_data[numerical_features] = numerical_imputer.fit_transform(train_data[numerical_features])\n",
        "\n",
        "# Impute missing values for categorical features\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "train_data[categorical_features] = categorical_imputer.fit_transform(train_data[categorical_features])\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "train_data = pd.get_dummies(train_data, columns=categorical_features)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = train_data.drop(columns=[\"DiagPeriodL90D\"])\n",
        "y = train_data[\"DiagPeriodL90D\"]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# SelectKBest with ANOVA F-statistic as the score function (for classification problems)\n",
        "k = 10\n",
        "selector = SelectKBest(score_func=f_classif, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X_train.columns[selected_feature_indices]\n",
        "\n",
        "# Create a DataFrame with selected features\n",
        "selected_features_df = pd.DataFrame(X_train_selected, columns=selected_feature_names)\n",
        "\n",
        "# Print the names of selected features\n",
        "print(\"Selected Features:\")\n",
        "print(selected_feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPnOGifTHh6o"
      },
      "source": [
        "# **Recursive Feature Elimination (RFE)**:\n",
        "\n",
        "RFE recursively removes the least important features based on a model's feature weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RsLBDXWHlLN",
        "outputId": "7d74e128-10d7-44c0-f9d8-f219c3e48163"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data (replace 'your_dataset.csv' with your actual dataset)\n",
        "train_data = pd.read_csv(\"training.csv\")\n",
        "\n",
        "# Drop the patient_id column\n",
        "train_data = train_data.drop(['patient_id'], axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "# Separate numerical and categorical features\n",
        "numerical_features = train_data.select_dtypes(include=['float64']).columns\n",
        "categorical_features = train_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "train_data[numerical_features] = numerical_imputer.fit_transform(train_data[numerical_features])\n",
        "\n",
        "# Impute missing values for categorical features\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "train_data[categorical_features] = categorical_imputer.fit_transform(train_data[categorical_features])\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "train_data = pd.get_dummies(train_data, columns=categorical_features)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = train_data.drop(columns=[\"DiagPeriodL90D\"])\n",
        "y = train_data[\"DiagPeriodL90D\"]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a base classifier (you can use any other classifier of your choice)\n",
        "base_classifier = RandomForestClassifier()\n",
        "\n",
        "# Create the RFE model and select 10 features\n",
        "n_features_to_select = 10\n",
        "rfe = RFE(estimator=base_classifier, n_features_to_select=n_features_to_select)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X_train.columns[rfe.support_]\n",
        "\n",
        "# Create a DataFrame with selected features\n",
        "selected_features_df = pd.DataFrame(X_train_rfe, columns=selected_feature_names)\n",
        "\n",
        "# Print the names of selected features\n",
        "print(\"Selected Features:\")\n",
        "print(selected_feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm-La47JHllt"
      },
      "source": [
        "LASSO Regression:\n",
        "\n",
        "L1 regularization (LASSO) can be used to encourage sparsity in feature weights, effectively performing feature selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QTMvA75HqDv",
        "outputId": "deda2ed3-3cc8-400e-bb0d-7621a50b8379"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data (replace 'your_dataset.csv' with your actual dataset)\n",
        "train_data = pd.read_csv(\"training.csv\")\n",
        "\n",
        "# Drop the patient_id column\n",
        "train_data = train_data.drop(['patient_id'], axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "# Separate numerical and categorical features\n",
        "numerical_features = train_data.select_dtypes(include=['float64']).columns\n",
        "categorical_features = train_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "train_data[numerical_features] = numerical_imputer.fit_transform(train_data[numerical_features])\n",
        "\n",
        "# Impute missing values for categorical features\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "train_data[categorical_features] = categorical_imputer.fit_transform(train_data[categorical_features])\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "train_data = pd.get_dummies(train_data, columns=categorical_features)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = train_data.drop(columns=[\"DiagPeriodL90D\"])\n",
        "y = train_data[\"DiagPeriodL90D\"]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "# Create the Lasso model\n",
        "lasso = Lasso(alpha=0.04)  # Adjust the alpha parameter based on the desired level of regularization\n",
        "\n",
        "# Fit the model to the training data\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the coefficients of the features\n",
        "feature_coefficients = pd.Series(lasso.coef_, index=X.columns)\n",
        "\n",
        "# Get the names of non-zero coefficient features (selected features)\n",
        "selected_feature_names = feature_coefficients[feature_coefficients != 0].index\n",
        "\n",
        "# Print the names of selected features\n",
        "print(\"Selected Features:\")\n",
        "print(selected_feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPzqnEOIHqq8"
      },
      "source": [
        "# Tree-based Methods:\n",
        "\n",
        "Decision trees and tree-based models can be analyzed for feature importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jK1GN5bAKf4Q",
        "outputId": "06abfcc7-3b9c-4baa-f9bb-905ac664881a"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your data (replace 'your_dataset.csv' with your actual dataset)\n",
        "train_data = pd.read_csv(\"training.csv\")\n",
        "\n",
        "# Drop the patient_id column\n",
        "train_data = train_data.drop(['patient_id'], axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "# Separate numerical and categorical features\n",
        "numerical_features = train_data.select_dtypes(include=['float64']).columns\n",
        "categorical_features = train_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "train_data[numerical_features] = numerical_imputer.fit_transform(train_data[numerical_features])\n",
        "\n",
        "# Impute missing values for categorical features\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "train_data[categorical_features] = categorical_imputer.fit_transform(train_data[categorical_features])\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "train_data = pd.get_dummies(train_data, columns=categorical_features)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = train_data.drop(columns=[\"DiagPeriodL90D\"])\n",
        "y = train_data[\"DiagPeriodL90D\"]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Get the names of features\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Create a DataFrame with feature names and their importance scores\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(50, 60))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Feature Importances')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nd3lQbsSLG_-",
        "outputId": "d13edb39-c9a0-46a9-9961-202d56ff14b6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Data\n",
        "df = pd.read_csv('training.csv')\n",
        "bmi_column = df['bmi']\n",
        "\n",
        "# 1. Missing Values\n",
        "missing_values = bmi_column.isnull().sum()\n",
        "print(f\"Number of missing values in BMI column: {missing_values}\")\n",
        "\n",
        "# 2. Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(bmi_column.dropna(), bins=30, kde=True)\n",
        "plt.title('BMI Distribution')\n",
        "plt.show()\n",
        "\n",
        "# 3. Summary Statistics\n",
        "summary_stats = bmi_column.describe()\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(summary_stats)\n",
        "\n",
        "# 4. Correlation with Other Features\n",
        "correlation_matrix = df.corr()\n",
        "bmi_correlation = correlation_matrix['bmi'].sort_values(ascending=False)\n",
        "print(\"\\nCorrelation with BMI:\")\n",
        "print(bmi_correlation)\n",
        "\n",
        "# 5. BMI Categories\n",
        "def categorize_bmi(bmi):\n",
        "    if bmi < 18.5:\n",
        "        return 'Underweight'\n",
        "    elif 18.5 <= bmi < 25:\n",
        "        return 'Normal Weight'\n",
        "    elif 25 <= bmi < 30:\n",
        "        return 'Overweight'\n",
        "    else:\n",
        "        return 'Obese'\n",
        "\n",
        "df['bmi_category'] = df['bmi'].apply(categorize_bmi)\n",
        "\n",
        "df['bmi_category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3xiciUs0d4SA",
        "outputId": "170a0463-23bc-4159-c9de-07ae1829dc95"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Data\n",
        "df = pd.read_csv('training.csv')\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Get the top 10 columns with the highest absolute correlation with 'bmi'\n",
        "top_columns = correlation_matrix['bmi'].abs().sort_values(ascending=False).head(11).index[1:]\n",
        "\n",
        "# Plot the heatmap for the top 10 columns\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(correlation_matrix[top_columns].loc[top_columns], annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.show()\n",
        "\n",
        "print(df['bmi'].isnull().sum())\n",
        "\n",
        "print(\"Top Col: \", top_columns)\n",
        "\n",
        "# Assuming X contains features and y contains BMI\n",
        "X = df[top_columns]\n",
        "y = df['bmi']\n",
        "\n",
        "# Identify rows with missing 'bmi' values\n",
        "missing_bmi_mask = y.isnull()\n",
        "\n",
        "# Separate the dataframe into features (X_train) and target (y_train) for non-missing values\n",
        "X_train, y_train = X[~missing_bmi_mask], y[~missing_bmi_mask]\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "# Create transformers for numerical columns\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # You can choose another strategy\n",
        "])\n",
        "\n",
        "# Use ColumnTransformer to apply the numerical transformer to the numerical columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "    ])\n",
        "\n",
        "# Build the model pipeline\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                        ('regressor', RandomForestRegressor())])\n",
        "\n",
        "# Train the model on the rows where 'bmi' is not missing\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict missing 'bmi' values in the entire dataset\n",
        "predicted_bmi = model.predict(X[missing_bmi_mask])\n",
        "\n",
        "# Fill missing 'bmi' values with predictions\n",
        "df.loc[missing_bmi_mask, 'bmi_filled'] = predicted_bmi\n",
        "\n",
        "# Print the original 'bmi' and the new 'bmi_filled' columns\n",
        "print(df[['bmi', 'bmi_filled']])\n",
        "\n",
        "# Create a new column 'merged_bmi' with the merged output of 'bmi' and 'bmi_filled'\n",
        "df['merged_bmi'] = df.apply(lambda row: row['bmi_filled'] if pd.notnull(row['bmi_filled']) else row['bmi'], axis=1)\n",
        "\n",
        "df.to_csv('training_bmi.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppEMCkyr-XH_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARddxZ_6aNI_"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your data (replace 'your_dataset.csv' with your actual dataset)\n",
        "train_data = pd.read_csv(\"training_Preprocessed.csv\")\n",
        "\n",
        "# Drop the patient_id column\n",
        "train_data.drop(columns=['patient_id'], inplace=True)\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = train_data.drop(columns=[\"DiagPeriodL90D\"])\n",
        "y = train_data[\"DiagPeriodL90D\"]\n",
        "\n",
        "# Handle missing values\n",
        "numerical_features = X.select_dtypes(include=['float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "X[numerical_features] = numerical_imputer.fit_transform(X[numerical_features])\n",
        "\n",
        "# Impute missing values for categorical features\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X[categorical_features] = categorical_imputer.fit_transform(X[categorical_features])\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "X = pd.get_dummies(X, columns=categorical_features)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Get the names of features\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Create a DataFrame with feature names and their importance scores\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 106))  # Adjust figure size as needed\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Feature Importances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfCSwfQqcAu1",
        "outputId": "d3b71726-4b9d-4e73-be7f-5d7519f95327"
      },
      "outputs": [],
      "source": [
        "# prompt: list the columns of dataset\n",
        "\n",
        "print(X_train.columns.to_list())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5KorxczdKqa",
        "outputId": "39adefef-85ab-42b9-c40f-d424b63ac839"
      },
      "outputs": [],
      "source": [
        "# Assuming 'train_data' is your DataFrame\n",
        "null_values = train_data.isnull().sum()\n",
        "print(\"Null values in each column:\")\n",
        "print(null_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uDegt8iaf76"
      },
      "source": [
        "LASSO Regression:\n",
        "\n",
        "L1 regularization (LASSO) can be used to encourage sparsity in feature weights, effectively performing feature selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBGCqsngafCo",
        "outputId": "e52fa6dd-d7e6-431e-f420-e90c5d2bace3"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data (replace 'your_dataset.csv' with your actual dataset)\n",
        "train_data = pd.read_csv(\"cleaned_dataset_encoded.csv\")\n",
        "\n",
        "# Drop the patient_id column\n",
        "train_data = train_data.drop(['patient_id'], axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "# Separate numerical and categorical features\n",
        "numerical_features = train_data.select_dtypes(include=['float64']).columns\n",
        "categorical_features = train_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "train_data[numerical_features] = numerical_imputer.fit_transform(train_data[numerical_features])\n",
        "\n",
        "# Impute missing values for categorical features\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "train_data[categorical_features] = categorical_imputer.fit_transform(train_data[categorical_features])\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "train_data = pd.get_dummies(train_data, columns=categorical_features)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = train_data.drop(columns=[\"DiagPeriodL90D\"])\n",
        "y = train_data[\"DiagPeriodL90D\"]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "# Create the Lasso model\n",
        "lasso = Lasso(alpha=0.04)  # Adjust the alpha parameter based on the desired level of regularization\n",
        "\n",
        "# Fit the model to the training data\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the coefficients of the features\n",
        "feature_coefficients = pd.Series(lasso.coef_, index=X.columns)\n",
        "\n",
        "# Get the names of non-zero coefficient features (selected features)\n",
        "selected_feature_names = feature_coefficients[feature_coefficients != 0].index\n",
        "\n",
        "# Print the names of selected features\n",
        "print(\"Selected Features:\")\n",
        "print(selected_feature_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik7TyePbxRDd",
        "outputId": "2b60ceb6-60da-4e17-ace7-09ce7d1a43f4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data (replace 'your_dataset.csv' with your actual dataset)\n",
        "train_data = pd.read_csv(\"final_rounded.csv\")\n",
        "# train_data_rounded = train_data.round(2)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = train_data.drop(columns=[\"DiagPeriodL90D\"])\n",
        "y = train_data[\"DiagPeriodL90D\"]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a base classifier (you can use any other classifier of your choice)\n",
        "base_classifier = RandomForestClassifier()\n",
        "\n",
        "# Create the RFE model and select 10 features\n",
        "n_features_to_select = 10\n",
        "rfe = RFE(estimator=base_classifier, n_features_to_select=n_features_to_select)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X_train.columns[rfe.support_]\n",
        "\n",
        "# Create a DataFrame with selected features\n",
        "selected_features_df = pd.DataFrame(X_train_rfe, columns=selected_feature_names)\n",
        "\n",
        "# Print the names of selected features\n",
        "print(\"Selected Features:\")\n",
        "print(selected_feature_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3Uz-ljUjvMIV",
        "outputId": "5abe1103-c97b-4118-9dec-5dc578ad1cd4"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Data\n",
        "df = pd.read_csv('cleaned_dataset_encoded.csv')\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Get the top 10 columns with the highest absolute correlation with 'bmi'\n",
        "top_columns = correlation_matrix['DiagPeriodL90D'].abs().sort_values(ascending=False).head(11).index[1:]\n",
        "\n",
        "# Plot the heatmap for the top 10 columns\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(correlation_matrix[top_columns].loc[top_columns], annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.show()\n",
        "\n",
        "print(df['DiagPeriodL90D'].isnull().sum())\n",
        "\n",
        "print(\"Top Col: \", top_columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/home/prital/Hackathon/cleaned_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  #bin edges\n",
        "age_labels = ['0-10', '11-20', '21-30', '31-40', '41-50','51-60','61-70', '71-80','81-90','90+']  \n",
        "\n",
        "df['age_group'] = pd.cut(df['patient_age'], bins=age_bins, labels=age_labels, right=False)\n",
        "\n",
        "print(df[['patient_age', 'age_group']].head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "bmi_bins = [0, 18.5, 25, 30, float('inf')]  \n",
        "bmi_labels = ['Underweight', 'Normal Weight', 'Overweight', 'Obese']  \n",
        "\n",
        "df['bmi_category'] = pd.cut(df['bmi_filled'], bins=bmi_bins, labels=bmi_labels, right=False)\n",
        "\n",
        "print(df[['bmi_filled', 'bmi_category']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "bmi_data = df[['bmi_filled']]\n",
        "\n",
        "num_clusters = 3\n",
        "\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "\n",
        "kmeans.fit(bmi_data)\n",
        "\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "\n",
        "df['bmi_cluster'] = cluster_labels\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(df['bmi_filled'], [0] * len(df), c=df['bmi_cluster'], cmap='viridis')\n",
        "plt.scatter(cluster_centers, [0] * num_clusters, marker='x', color='red', label='Cluster Centers')\n",
        "plt.xlabel('BMI')\n",
        "plt.ylabel('Cluster')\n",
        "plt.title('K-Means Clustering of BMI')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(df[['bmi_filled', 'bmi_cluster']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "age_data = df[['patient_age']]\n",
        "\n",
        "num_clusters = 3\n",
        "\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "\n",
        "kmeans.fit(age_data)\n",
        "\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "df['age_cluster'] = cluster_labels\n",
        "\n",
        "# Visualize \n",
        "plt.scatter(df['patient_age'], [0] * len(df), c=df['age_cluster'], cmap='viridis')\n",
        "plt.scatter(cluster_centers, [0] * num_clusters, marker='x', color='red', label='Cluster Centers')\n",
        "plt.xlabel('Patient Age')\n",
        "plt.ylabel('Cluster')\n",
        "plt.title('K-Means Clustering of Patient Age')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(df[['patient_age', 'age_cluster']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "income_data = df[['patient_income']]\n",
        "\n",
        "\n",
        "num_clusters = 3\n",
        "\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "\n",
        "kmeans.fit(income_data)\n",
        "\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "df['income_cluster'] = cluster_labels\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(df['patient_income'], [0] * len(df), c=df['income_cluster'], cmap='viridis')\n",
        "plt.scatter(cluster_centers, [0] * num_clusters, marker='x', color='red', label='Cluster Centers')\n",
        "plt.xlabel('Patient Income')\n",
        "plt.ylabel('Cluster')\n",
        "plt.title('K-Means Clustering of Patient Income')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(df[['patient_income', 'income_cluster']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "cluster_diagnosis_df = df[['age_cluster', 'DiagPeriodL90D']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cancer_cases_in_clusters = cluster_diagnosis_df.groupby(['age_cluster', 'DiagPeriodL90D']).size().unstack(fill_value=0)\n",
        "print(cancer_cases_in_clusters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cancer_cases_in_clusters.plot(kind='bar', stacked=True)\n",
        "plt.title('Distribution of Cancer Cases in Clusters')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_counts = cluster_diagnosis_df['age_cluster'].value_counts()\n",
        "cancer_proportion_in_clusters = cancer_cases_in_clusters / cluster_counts[:, None]\n",
        "print(cancer_proportion_in_clusters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cancer_proportion_in_clusters.plot(kind='bar', stacked=True)\n",
        "plt.title('Proportion of Cancer Cases in Clusters')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Proportion')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "chi2, p, _, _ = chi2_contingency(cancer_cases_in_clusters)\n",
        "print(f'Chi-Square Value: {chi2}, p-value: {p}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0v9soyCDUOW",
        "outputId": "d52486c7-d0e7-40d0-97ec-ad41fdc17feb"
      },
      "outputs": [],
      "source": [
        "# Keras\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "df = pd.read_csv('final2.csv')\n",
        "\n",
        "target_variable = 'DiagPeriodL90D'\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "#  predictors\n",
        "predictors = list(df.columns[df.columns != target_variable])\n",
        "\n",
        "X_train = train[predictors].values\n",
        "y_train = train[target_variable].values\n",
        "X_test = test[predictors].values\n",
        "y_test = test[target_variable].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Keras model\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# predictions\n",
        "predictions = model.predict(X_test)\n",
        "predictions_binary = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, predictions_binary)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, predictions_binary))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "df = pd.read_csv(\"training//training.csv\")\n",
        "print(df.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# count of missing values in each column\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"training//final2.csv\")\n",
        "\n",
        "df_rounded = df.round(2)\n",
        "\n",
        "# to check rounded values\n",
        "df_rounded.to_csv(\"training//final_rounded.csv\")\n",
        "print(df_rounded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# checking if there is any row with non malignant neoplasm\n",
        "\n",
        "df = pd.read_csv(\"training//training_bmi_nlp.csv\")\n",
        "\n",
        "rows_without_malignant_neoplasm = ~df['processed_text'].str.contains('malignant neoplasm', case=False)\n",
        "\n",
        "print(df[rows_without_malignant_neoplasm])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NLP pipeline for description preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df = pd.read_csv('training//training_bmi.csv')\n",
        "dataf=df.copy()\n",
        "\n",
        "# mapping dictionary for specific replacements\n",
        "correction_dict = {\n",
        "    'Malig': 'Malignant',\n",
        "    'neoplm': 'neoplasm',\n",
        "    'unsp': 'unspecified',\n",
        "    'ovrlp': 'overlap'\n",
        "}\n",
        "\n",
        "# Text Cleaning\n",
        "def clean_text(text):\n",
        "    # Spell correction for whole words only\n",
        "    words = text.split()\n",
        "    corrected_words = [correction_dict.get(word, word) for word in words]\n",
        "    text = ' '.join(corrected_words)\n",
        "    \n",
        "    # Remove unnecessary characters, symbols, or special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert the text to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "dataf['cleaned_text'] = dataf['breast_cancer_diagnosis_desc'].apply(clean_text)\n",
        "\n",
        "# Tokenization\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "dataf['tokenized_text'] = dataf['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "dataf['filtered_tokens'] = dataf['tokenized_text'].apply(remove_stopwords)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "dataf['lemmatized_tokens'] = dataf['filtered_tokens'].apply(lemmatize_tokens)\n",
        "# Save the processed text in a new column\n",
        "dataf['processed_text'] = dataf['lemmatized_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Define the words to remove\n",
        "words_to_remove = ['malignant', 'neoplasm', 'site', 'breast', 'female']\n",
        "\n",
        "# Create a new column with the specified words removed\n",
        "dataf['imp_desc'] = dataf['processed_text'].replace('|'.join(words_to_remove), '', regex=True).replace('\\s+', ' ', regex=True)\n",
        "\n",
        "# Remove rows with empty strings in 'imp_desc'\n",
        "dataf = dataf[dataf['imp_desc'].str.strip() != '']\n",
        "\n",
        "# Function to remove repeated words in a block and maintain order\n",
        "def remove_repeated_words(text):\n",
        "    words = text.split()\n",
        "    unique_words = []\n",
        "    \n",
        "    for word in words:\n",
        "        if word not in unique_words:\n",
        "            unique_words.append(word)\n",
        "    \n",
        "    return ' '.join(unique_words)\n",
        "\n",
        "# Apply the function to the 'imp_desc' column\n",
        "dataf['affected_site'] = dataf['imp_desc'].apply(remove_repeated_words)\n",
        "df['processed_text'] = dataf['processed_text']\n",
        "df['affected_site'] = dataf['affected_site'] \n",
        "# DataFrame with the new column\n",
        "print(dataf[['breast_cancer_diagnosis_desc', 'affected_site']])\n",
        "df.to_csv('training//training_bmi_nlp.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"The features that have low p-values in the ANOVA test are the ones that show significant \n",
        "differences across different categories of the patient_race column. These features are likely to be informative \n",
        "for predicting or imputing values in the patient_race column\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "df = pd.read_csv('training//training_bmi.csv')\n",
        "numerical_columns = [\n",
        "    'patient_zip3', 'patient_age', 'population', 'density', 'age_median', 'age_under_10',\n",
        "    'age_10_to_19', 'age_20s', 'age_30s', 'age_40s', 'age_50s', 'age_60s', 'age_70s', 'age_over_80',\n",
        "    'male', 'female', 'married', 'divorced', 'never_married', 'widowed', 'family_size',\n",
        "    'family_dual_income', 'income_household_median', 'income_household_under_5',\n",
        "    'income_household_5_to_10', 'income_household_10_to_15', 'income_household_15_to_20',\n",
        "    'income_household_20_to_25', 'income_household_25_to_35', 'income_household_35_to_50',\n",
        "    'income_household_50_to_75', 'income_household_75_to_100', 'income_household_100_to_150',\n",
        "    'income_household_150_over', 'income_household_six_figure', 'income_individual_median',\n",
        "    'home_ownership', 'housing_units', 'home_value', 'rent_median', 'rent_burden',\n",
        "    'education_less_highschool', 'education_highschool', 'education_some_college',\n",
        "    'education_bachelors', 'education_graduate', 'education_college_or_above',\n",
        "    'education_stem_degree', 'labor_force_participation', 'unemployment_rate',\n",
        "    'self_employed', 'farmer', 'race_white', 'race_black', 'race_asian', 'race_native',\n",
        "    'race_pacific', 'race_other', 'race_multiple', 'hispanic'\n",
        "]\n",
        "\n",
        "# DataFrame to store the results\n",
        "anova_results = pd.DataFrame(columns=['Feature', 'F-statistic', 'P-value'])\n",
        "\n",
        "for feature in numerical_columns:\n",
        "    # Filtering out NaN values\n",
        "    non_nan_data = df[[feature, 'patient_race']].dropna()\n",
        "    \n",
        "    # ANOVA test\n",
        "    group_by_race = non_nan_data.groupby('patient_race')[feature]\n",
        "    anova_result = f_oneway(*[group.values for name, group in group_by_race])\n",
        "    \n",
        "    # results\n",
        "    anova_results = anova_results.append({\n",
        "        'Feature': feature,\n",
        "        'F-statistic': anova_result.statistic,\n",
        "        'P-value': anova_result.pvalue\n",
        "    }, ignore_index=True)\n",
        "\n",
        "print(anova_results.sort_values(by='P-value'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Patient_race null value filling \n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"training//training.csv\")\n",
        "\n",
        "# Feature Selection\n",
        "numerical_features = [\n",
        "    'race_white', 'race_black', 'race_asian', 'race_native',\n",
        "    'race_pacific', 'race_other', 'race_multiple', 'hispanic', 'patient_zip3'\n",
        "]\n",
        "\n",
        "categorical_feature = 'patient_race'\n",
        "\n",
        "\n",
        "#DataFrame with rows containing null 'patient_race'\n",
        "df_predict = df[df[categorical_feature].isnull()]\n",
        "\n",
        "# Droping rows where the target variable has null values\n",
        "training_data = df.dropna(subset=[categorical_feature] + numerical_features)\n",
        "\n",
        "if not df_predict.empty:\n",
        "    X = training_data[numerical_features]\n",
        "    y = training_data[categorical_feature]\n",
        "\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    # Code to find accuracy\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(training_data[numerical_features], training_data[categorical_feature], test_size=0.2, shuffle=True)\n",
        "    # model = RandomForestClassifier(random_state=42)\n",
        "    # model.fit(X_train, y_train)\n",
        "    # y_pred = model.predict(X_test)\n",
        "    # accuracy = accuracy_score(y_test, y_pred)\n",
        "    # print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "# Prediction\n",
        "    print(\"Before Prediction:\")\n",
        "    print(\"Total nulls: \", df['patient_race'].isnull().sum())\n",
        "    print(\"count\")\n",
        "    print(df['patient_race'].value_counts())\n",
        "\n",
        "    # 'patient_race_filled' has the predicted values\n",
        "    df['patient_race_filled'] = df['patient_race']\n",
        "\n",
        "    # Replacing null values in 'patient_race_filled' with the predicted values\n",
        "    df.loc[df_predict.index, 'patient_race_filled'] = model.predict(df_predict[numerical_features])\n",
        "\n",
        "    print(\"\\nAfter Prediction:\")\n",
        "    print(\"Total nulls: \", df['patient_race_filled'].isnull().sum())\n",
        "    print(\"count\")\n",
        "    print(df['patient_race_filled'].value_counts())\n",
        "\n",
        "\n",
        "    df.to_csv(\"training//training_race_filled.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"training//cleaned_dataset.csv\")\n",
        "\n",
        "# column for one-hot encoding\n",
        "df = data[['patient_gender']]\n",
        "\n",
        "# Performing one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=['patient_gender'], prefix=['patient'])\n",
        "\n",
        "# Merging the data\n",
        "data_encoded = pd.concat([data, df_encoded], axis=1)\n",
        "\n",
        "print(data_encoded)\n",
        "\n",
        "# Save\n",
        "data_encoded.to_csv(\"training//cleaned_dataset_encoded.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"training//cleaned_dataset_encoded.csv\")\n",
        "\n",
        "# column for one-hot encoding\n",
        "df = data[['patient_race_filled']]\n",
        "\n",
        "# Performing one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=['patient_race_filled'], prefix=['race'])\n",
        "\n",
        "# Merging data\n",
        "data_encoded = pd.concat([data, df_encoded], axis=1)\n",
        "\n",
        "print(data_encoded)\n",
        "\n",
        "# Save\n",
        "data_encoded.to_csv(\"training//cleaned_dataset_encoded.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"training//cleaned_dataset_encoded.csv\")\n",
        "\n",
        "#column for one-hot encoding\n",
        "df = data[['payer_type']]\n",
        "\n",
        "# Performing one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=['payer_type'], prefix=['payer'])\n",
        "\n",
        "# Merging data\n",
        "data_encoded = pd.concat([data, df_encoded], axis=1)\n",
        "\n",
        "print(data_encoded)\n",
        "\n",
        "# Save\n",
        "data_encoded.to_csv(\"training//cleaned_dataset_encoded.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trying TFIDF vectorizer for affected_site column encoding but not used for final data\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "data = pd.read_csv(\"training//cleaned_dataset.csv\")\n",
        "df = data['affected_site']\n",
        "data['affected_site'].fillna('unspecified', inplace=True)\n",
        "\n",
        "print(df)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "df_encoded = tfidf_vectorizer.fit_transform(df)\n",
        "print(df_encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Frequency encoding diagnosis codes\n",
        "\n",
        "data = pd.read_csv(\"training//final.csv\")\n",
        "\n",
        "# Filling missing values\n",
        "data['breast_cancer_diagnosis_code'].fillna('0', inplace=True)\n",
        "\n",
        "# column for frequency encoding\n",
        "df = data['breast_cancer_diagnosis_code']\n",
        "\n",
        "\n",
        "frequency_encoding = df.value_counts(normalize=True).to_dict()\n",
        "\n",
        "df_encoded = df.map(frequency_encoding)\n",
        "\n",
        "data['breast_cancer_diagnosis_code_encoded'] = df_encoded\n",
        "\n",
        "data.to_csv(\"training//final1.csv\", index=False)\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Frequency encoding diagnosis codes\n",
        "\n",
        "data = pd.read_csv(\"training//final1.csv\")\n",
        "\n",
        "# Fill missing values\n",
        "data['metastatic_cancer_diagnosis_code'].fillna('0', inplace=True)\n",
        "\n",
        "# column for frequency encoding\n",
        "df = data['metastatic_cancer_diagnosis_code']\n",
        "\n",
        "frequency_encoding = df.value_counts(normalize=True).to_dict()\n",
        "\n",
        "df_encoded = df.map(frequency_encoding)\n",
        "\n",
        "data['metastatic_cancer_diagnosis_code_encoded'] = df_encoded\n",
        "\n",
        "data.to_csv(\"training//final2.csv\", index=False)\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Selection\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(\"training//final_rounded.csv\")\n",
        "\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = train_data.drop(columns=[\"DiagPeriodL90D\"])\n",
        "y = train_data[\"DiagPeriodL90D\"]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a base classifier\n",
        "base_classifier = RandomForestClassifier()\n",
        "\n",
        "# Create the RFE model and select 10 features\n",
        "n_features_to_select = 10\n",
        "rfe = RFE(estimator=base_classifier, n_features_to_select=n_features_to_select)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X_train.columns[rfe.support_]\n",
        "\n",
        "# Create a DataFrame with selected features\n",
        "selected_features_df = pd.DataFrame(X_train_rfe, columns=selected_feature_names)\n",
        "\n",
        "# Print the names of selected features\n",
        "print(\"Selected Features:\")\n",
        "print(selected_feature_names)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
